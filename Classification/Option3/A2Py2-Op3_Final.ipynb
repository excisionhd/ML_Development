{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import glob\n",
    "import keras\n",
    "import time\n",
    "from keras import backend as K\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import categorical_crossentropy\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import CSVLogger, EarlyStopping, TensorBoard, ModelCheckpoint \n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from scipy.misc import imread, imresize\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras_vggface.vggface import VGGFace\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.layers import Input, Flatten, Dense\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.utils import to_categorical\n",
    "from numpy import genfromtxt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read Data\n",
    "my_data = pd.read_csv('train_data.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper to show image\n",
    "def plotImage(image):\n",
    "    f, axarr = plt.subplots(1,2)\n",
    "    axarr[0].imshow(image)\n",
    "    axarr[0].grid()\n",
    "    axarr[0].set_title('Image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python36\\site-packages\\numpy\\core\\fromnumeric.py:51: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  return getattr(obj, method)(*args, **kwds)\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "#Create np array of the images, reshape to (48,48) then convert to 3D\n",
    "images = []\n",
    "\n",
    "for index, row in my_data.iterrows():\n",
    "    image = np.reshape(row, (48,48))\n",
    "    image = imresize(image, (224,224,3))\n",
    "    image = np.reshape(image, image.shape + (1,))\n",
    "    images.append(image)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to NP array\n",
    "X_train = np.asarray(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read training data\n",
    "targets = pd.read_csv('train_target.csv', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, targets, test_size=0.10, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply padding to images to feed into network.\n",
    "pad = ((0,0),)*3 + ((0,2),)\n",
    "X_train = np.pad(X_train, pad, 'constant', constant_values = 0)\n",
    "X_test = np.pad(X_test, pad, 'constant', constant_values = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14557, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to One-hot encoding\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv1_1 (Conv2D)             (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "conv1_2 (Conv2D)             (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "pool1 (MaxPooling2D)         (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2_1 (Conv2D)             (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "conv2_2 (Conv2D)             (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "pool2 (MaxPooling2D)         (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv3_1 (Conv2D)             (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv3_2 (Conv2D)             (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "conv3_3 (Conv2D)             (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "pool3 (MaxPooling2D)         (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv4_1 (Conv2D)             (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "conv4_2 (Conv2D)             (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "conv4_3 (Conv2D)             (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "pool4 (MaxPooling2D)         (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv5_1 (Conv2D)             (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "conv5_2 (Conv2D)             (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "conv5_3 (Conv2D)             (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "pool5 (MaxPooling2D)         (None, 7, 7, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Create the VGG Face model\n",
    "vgg_face_model = VGGFace(model = 'vgg16', include_top = False, weights='vggface', input_shape=(224,224,3))\n",
    "    \n",
    "vgg_face_model.summary()\n",
    "\n",
    "#Freeze all layers\n",
    "for layer in vgg_face_model.layers[:-1]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add the fully connected layers and classifier\n",
    "from keras import regularizers\n",
    "LL = vgg_face_model.get_layer('pool5').output\n",
    "x = Flatten(name='flatten')(LL)\n",
    "x = Dense(64,name = 'fc7')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "out = Dense(3, activation='softmax',name='classifier')(x)\n",
    "custom_vgg_face_model = Model(vgg_face_model.input, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specify optimizer, loss function, and metrics to track\n",
    "sgd = SGD(lr=0.0003, decay=1e-5, momentum=0.9)\n",
    "custom_vgg_face_model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14557 samples, validate on 1618 samples\n",
      "Epoch 1/75\n",
      "14557/14557 [==============================] - 107s 7ms/step - loss: 5.2842 - acc: 0.6414 - val_loss: 4.2213 - val_acc: 0.7169\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.71693, saving model to vgg_face_gender_weights_improvment-01-0.72.hdf5\n",
      "Epoch 2/75\n",
      "14557/14557 [==============================] - 100s 7ms/step - loss: 4.1419 - acc: 0.7220 - val_loss: 3.8645 - val_acc: 0.7379\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.71693 to 0.73795, saving model to vgg_face_gender_weights_improvment-02-0.74.hdf5\n",
      "Epoch 3/75\n",
      "14557/14557 [==============================] - 100s 7ms/step - loss: 3.7084 - acc: 0.7503 - val_loss: 3.3536 - val_acc: 0.7756\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.73795 to 0.77565, saving model to vgg_face_gender_weights_improvment-03-0.78.hdf5\n",
      "Epoch 4/75\n",
      "14557/14557 [==============================] - 100s 7ms/step - loss: 3.5047 - acc: 0.7670 - val_loss: 3.5528 - val_acc: 0.7528\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.77565\n",
      "Epoch 5/75\n",
      "14557/14557 [==============================] - 100s 7ms/step - loss: 3.3922 - acc: 0.7729 - val_loss: 3.1909 - val_acc: 0.7837\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.77565 to 0.78368, saving model to vgg_face_gender_weights_improvment-05-0.78.hdf5\n",
      "Epoch 6/75\n",
      "14557/14557 [==============================] - 101s 7ms/step - loss: 3.1230 - acc: 0.7893 - val_loss: 2.9525 - val_acc: 0.8022\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.78368 to 0.80222, saving model to vgg_face_gender_weights_improvment-06-0.80.hdf5\n",
      "Epoch 7/75\n",
      "14557/14557 [==============================] - 101s 7ms/step - loss: 3.0719 - acc: 0.7930 - val_loss: 3.0326 - val_acc: 0.7973\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.80222\n",
      "Epoch 8/75\n",
      "14557/14557 [==============================] - 101s 7ms/step - loss: 3.0241 - acc: 0.7973 - val_loss: 3.1078 - val_acc: 0.7917\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.80222\n",
      "Epoch 9/75\n",
      "14557/14557 [==============================] - 101s 7ms/step - loss: 2.8776 - acc: 0.8065 - val_loss: 3.0700 - val_acc: 0.7954\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.80222\n",
      "Epoch 10/75\n",
      "14557/14557 [==============================] - 101s 7ms/step - loss: 2.8616 - acc: 0.8080 - val_loss: 3.4361 - val_acc: 0.7701\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.80222\n",
      "Epoch 11/75\n",
      "14557/14557 [==============================] - 100s 7ms/step - loss: 2.8174 - acc: 0.8107 - val_loss: 3.1085 - val_acc: 0.7899\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.80222\n",
      "Epoch 12/75\n",
      "10592/14557 [====================>.........] - ETA: 24s - loss: 2.5994 - acc: 0.8241 ETA: 26s - loss: 2.60"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-8449305b970b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#fit(x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mcustom_vgg_face_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m75\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1042\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2659\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2660\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2661\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2662\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2663\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2630\u001b[0m                                 session)\n\u001b[1;32m-> 2631\u001b[1;33m         \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2632\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1399\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1400\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# fine-tune the model\n",
    "filepath=\"vgg_face_gender_weights_improvment-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callback_list = [checkpoint]\n",
    "custom_vgg_face_model.fit(x=X_train,y=y_train, batch_size=32, epochs=75, verbose=1, callbacks=callback_list, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "#load weights if needed\n",
    "custom_vgg_face_model.load_weights(\"vgg_face_gender_weights_improvment-06-0.80.hdf5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read file names from sample_submissions\n",
    "test_data = pd.read_csv('test_data.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python36\\site-packages\\numpy\\core\\fromnumeric.py:51: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  return getattr(obj, method)(*args, **kwds)\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "#Create np array of testing images\n",
    "test_images = []\n",
    "\n",
    "for index, row in test_data.iterrows():\n",
    "    im = np.reshape(row, (48,48))\n",
    "    im = imresize(im, (224,224,3))\n",
    "    im = np.reshape(im, im.shape + (1,))\n",
    "    test_images.append(im)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to np array\n",
    "test_images2 = np.asarray(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply padding to test images for predictions\n",
    "pad = ((0,0),)*3 + ((0,2),)\n",
    "test_images2 = np.pad(test_images2, pad, 'constant', constant_values = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate predictions\n",
    "predictions = custom_vgg_face_model.predict(test_images2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get argmax of predictions\n",
    "arg_predictions = np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create df of predictions\n",
    "emotion = pd.DataFrame(arg_predictions) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read file names from sample_submissions\n",
    "submission = pd.read_csv('sample-submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create final submission\n",
    "result = pd.concat([submission, emotion],axis=1)\n",
    "result.columns = ['Id', 'C1', 'Category']\n",
    "result = result.drop(columns=['C1'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save results to CSV\n",
    "result.to_csv('answers_op3_vgg_face_2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 2: SVM w/ Landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import glob\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import dlib\n",
    "import itertools\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specify classes, initialize DLIB detector and predictor\n",
    "emotions = [\"0\", \"1\", \"2\"] \n",
    "clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "#Create SVM model\n",
    "clf = SVC(cache_size=200, C=1, gamma = 0.001, class_weight=None,  kernel='linear',\n",
    "  probability=False, random_state=None, \n",
    "  tol=0.001, verbose=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read data, and concatenate the dataframes\n",
    "my_data = pd.read_csv('train_data.csv', header=None)\n",
    "labels = pd.read_csv('train_target.csv', header=None)\n",
    "labels.rename(columns={labels.columns[0]:'Target'}, inplace=True)\n",
    "\n",
    "dataframe = pd.concat([my_data, labels],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python36\\site-packages\\numpy\\core\\fromnumeric.py:51: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  return getattr(obj, method)(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "#Split training and testing 80% and 20% respectively\n",
    "#Will split into classification directories and save images into a folder.\n",
    "data_size = len(dataframe.index)\n",
    "train_size = dataframe - (data_size * 0.2)\n",
    "count = 0\n",
    "\n",
    "for index, row in dataframe.iterrows():\n",
    "    image = np.reshape(row[:-1], (48,48))\n",
    "    target = str(row['Target'])\n",
    "    str_index = str(index)\n",
    "\n",
    "    if count<train_size:\n",
    "        \n",
    "        if os.path.isdir('data/train/' + target):\n",
    "            cv2.imwrite('data/train/' + target + '/' + str_index + '.jpg', image)\n",
    "        else:\n",
    "            os.makedirs('data/train/'+ target)\n",
    "            cv2.imwrite('data/train/' + target + '/' +  str_index + '.jpg', image)\n",
    "            \n",
    "        count = count + 1\n",
    "    \n",
    "    #For the last 20% of data, copy to validation with same logic as previous.\n",
    "    else:\n",
    "        if os.path.isdir('data/validation/' + target):\n",
    "            cv2.imwrite('data/validation/' + target + '/' + str_index + '.jpg', image)\n",
    "        else:\n",
    "            os.makedirs('data/validation/'+ target)\n",
    "            cv2.imwrite('data/validation/' + target + '/' + str_index + '.jpg', image)\n",
    "            \n",
    "        count = count + 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read testing Data\n",
    "testdf = pd.read_csv('test_data.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python36\\site-packages\\numpy\\core\\fromnumeric.py:51: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  return getattr(obj, method)(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "#Create folder of images for testing data\n",
    "for index, row in testdf.iterrows():\n",
    "    image = np.reshape(row, (48,48))\n",
    "    str_index = str(index)\n",
    "    \n",
    "    cv2.imwrite('data/kaggle_test/' + str(index) + \".jpg\", image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save all images to one folder\n",
    "for index, row in my_data.iterrows():\n",
    "    image = np.reshape(row[:-1], (48,48))\n",
    "    target = str(row['Target'])\n",
    "    str_index = str(index)\n",
    "    \n",
    "    cv2.imwrite('data/all/' + target + \"/\" + str_index + \".jpg\", image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### These functions were built with reference from http://www.paulvangent.com/2016/08/05/emotion-recognition-using-facial-landmarks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function retrieves files using file globbing\n",
    "def get_files(emotion): \n",
    "    files = glob.glob(\"data\\\\all\\\\%s\\\\*\" %emotion)\n",
    "    random.shuffle(files)\n",
    "    training = files[:int(len(files)*0.8)] \n",
    "    validation = files[-int(len(files)*0.2):]\n",
    "    return training, validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function retrieves test files using file globbing\n",
    "def get_test_files():\n",
    "    files = glob.glob(\"data\\\\kaggle_test\\\\**\")\n",
    "    test_data = files[:]\n",
    "    return test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize data dictionary to store landmarks\n",
    "data = {} \n",
    "\n",
    "#Function utilizes dlib to create landmarks, vectorize them, and store them into the dictionary.  (See url for explanation of arithmetic)\n",
    "def get_landmarks(image):\n",
    "    detections = detector(image, 1)\n",
    "    for k,d in enumerate(detections): #For all detected face instances individually\n",
    "        shape = predictor(image, d) #Draw Facial Landmarks with the predictor class\n",
    "        xlist = []\n",
    "        ylist = []\n",
    "        #for i in range(1,68): #Store X and Y coordinates in two lists\n",
    "            #xlist.append(float(shape.part(i).x))\n",
    "            #ylist.append(float(shape.part(i).y))\n",
    "        for p in shape.parts():\n",
    "            xlist.append(float(p.x))\n",
    "            ylist.append(float(p.y))\n",
    "        xmean = np.mean(xlist)\n",
    "        ymean = np.mean(ylist)\n",
    "        xcentral = [(x-xmean) for x in xlist]\n",
    "        ycentral = [(y-ymean) for y in ylist]\n",
    "        landmarks_vectorized = []\n",
    "        for x, y, w, z in zip(xcentral, ycentral, xlist, ylist):\n",
    "            landmarks_vectorized.append(w)\n",
    "            landmarks_vectorized.append(z)\n",
    "            meannp = np.asarray((ymean,xmean))\n",
    "            coornp = np.asarray((z,w))\n",
    "            dist = np.linalg.norm(coornp-meannp)\n",
    "            landmarks_vectorized.append(dist)\n",
    "            landmarks_vectorized.append((math.atan2(y, x)*360)/(2*math.pi))\n",
    "        data['landmarks_vectorized'] = landmarks_vectorized\n",
    "    if len(detections) < 1:\n",
    "        data['landmarks_vestorized'] = \"error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function returns training and testing dataset and labels along with vectorized landmarks\n",
    "def make_sets():\n",
    "    training_data = []\n",
    "    training_labels = []\n",
    "    prediction_data = []\n",
    "    prediction_labels = []\n",
    "    for emotion in emotions:\n",
    "        print(\" Working on %s\" %emotion)\n",
    "        training, prediction = get_files(emotion)\n",
    "        for item in training:\n",
    "            image = cv2.imread(item) \n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) \n",
    "            gray = cv2.resize(gray, (128,128))\n",
    "            clahe_image = clahe.apply(gray)\n",
    "            get_landmarks(clahe_image)\n",
    "            if data['landmarks_vectorized'] == \"error\":\n",
    "                print(\"no face detected on this one\")\n",
    "            else:\n",
    "                training_data.append(data['landmarks_vectorized'])\n",
    "                training_labels.append(emotions.index(emotion))\n",
    "        for item in prediction:\n",
    "            image = cv2.imread(item)\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            gray = cv2.resize(gray, (128,128))\n",
    "            clahe_image = clahe.apply(gray) \n",
    "            get_landmarks(clahe_image)\n",
    "            if data['landmarks_vectorized'] == \"error\":\n",
    "                print(\"No Face\")\n",
    "            else:\n",
    "                prediction_data.append(data['landmarks_vectorized'])\n",
    "                prediction_labels.append(emotions.index(emotion))\n",
    "    return training_data, training_labels, prediction_data, prediction_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function builts kaggle_test set and creates landmarks to pass into SVM\n",
    "def make_test_set():\n",
    "    test_data = []\n",
    "    t_data = get_test_files()\n",
    "    \n",
    "    for item in t_data:\n",
    "            image = cv2.imread(item)\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) \n",
    "            clahe_image = clahe.apply(gray)\n",
    "            get_landmarks(clahe_image)\n",
    "            if data['landmarks_vectorized'] == \"error\":\n",
    "                print(\"No Face\")\n",
    "            else:\n",
    "                test_data.append(data['landmarks_vectorized']) \n",
    "                \n",
    "    return test_data\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import pickle\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating datasets 0\n",
      " Working on 0\n",
      " Working on 1\n",
      " Working on 2\n",
      "Training SVM Linear #0\n",
      "[LibSVM]Obtaining accuracies Trial: 0\n",
      "linear:  0.7771251931993818\n",
      "Generating datasets 1\n",
      " Working on 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-93-83f4fd5d6dc4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_iters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Generating datasets %s\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mtraining_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprediction_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprediction_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_sets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mnpar_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mnpar_trainlabs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-91-5eb668395ced>\u001b[0m in \u001b[0;36mmake_sets\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[0mgray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[0mclahe_image\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclahe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m             \u001b[0mget_landmarks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclahe_image\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'landmarks_vectorised'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"error\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"no face detected on this one\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-52-bd3690c5e523>\u001b[0m in \u001b[0;36mget_landmarks\u001b[1;34m(image)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_landmarks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mdetections\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdetector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdetections\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#For all detected face instances individually\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#Draw Facial Landmarks with the predictor class\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mxlist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "accur_lin = []\n",
    "num_iters = 2\n",
    "\n",
    "#Create dataset, train the model.\n",
    "for i in range(0,num_iters):\n",
    "    print(\"Generating datasets %s\" %i) \n",
    "    training_data, training_labels, prediction_data, prediction_labels = make_sets()\n",
    "    npar_train = np.array(training_data) \n",
    "    npar_trainlabs = np.array(training_labels)\n",
    "    print(\"Training SVM Linear #%s\" %i) \n",
    "\n",
    "    scaling = MinMaxScaler(feature_range=(0,1)).fit(npar_train)\n",
    "    npar_train = scaling.transform(npar_train)\n",
    "    clf.fit(npar_train, training_labels)\n",
    "    pickle.dump(clf, open('SVM_' + str(i) + '.sav', 'wb'))\n",
    "    print(\"Obtaining accuracies Trial: %s\" %i) \n",
    "    npar_pred = np.array(prediction_data)\n",
    "    npar_pred = scaling.transform(npar_pred)\n",
    "    pred_lin = clf.score(npar_pred, prediction_labels)\n",
    "    print(\"linear: \", pred_lin)\n",
    "    accur_lin.append(pred_lin)\n",
    "print(\"Mean value lin svm: %s\" %np.mean(accur_lin)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get testing_data\n",
    "testing_data = make_test_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to np array\n",
    "tt_data = np.array(testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale the data before passing into SVM\n",
    "scaling = MinMaxScaler(feature_range=(0,1)).fit(tt_data)\n",
    "predictions_scaled = scaling.transform(tt_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate predictions\n",
    "predictions = clf.predict(predictions_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DescribeResult(nobs=3965, minmax=(0, 2), mean=0.9520807061790668, variance=0.5572389844031338, skewness=0.07762634686324128, kurtosis=-1.2023994143689478)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import stats\n",
    "stats.describe(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate DF of predictions\n",
    "resultsdf = pd.DataFrame(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read file names from sample_submissions, create final DF\n",
    "submission2 = pd.read_csv('sample-submission.csv')\n",
    "result = pd.concat([submission2, resultsdf],axis=1)\n",
    "result.columns = ['Id', 'C1', 'Category']\n",
    "result.drop(columns=['C1'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Working on 0\n",
      " Working on 1\n",
      " Working on 2\n"
     ]
    }
   ],
   "source": [
    "training_data, training_labels, prediction_data, prediction_labels = make_sets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
